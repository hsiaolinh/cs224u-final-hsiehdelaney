{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96b8fdf8",
   "metadata": {},
   "source": [
    "### Import data csv and normalize human evaluation scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035caa9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "df = pd.read_csv(\"focal_ela_science.csv\", encoding = \"UTF-8\")\n",
    "\n",
    "# def z_score(vals):\n",
    "#     vals - vals.mean())/vals.std(ddof=0)\n",
    "\n",
    "# rescaling D1 score (human evaluation of uptake) from 1-4 to 0-1\n",
    "# adding the normed values back to the dataframe\n",
    "df = df.assign(normed_D1 = (df[\"D1\"]-1)/3)\n",
    "df = df.assign(z_D1 = stats.zscore(df[\"D1\"], nan_policy='omit'))\n",
    "\n",
    "# cleaning URL into uuid\n",
    "df = df.assign(uuid = df.URL.apply(lambda x: x[39:]))\n",
    "# df['uuid'] = df.URL.apply(lambda x: x[39:])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c24202d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting conversations from the XML files\n",
    "\n",
    "# for c in conversations:\n",
    "#     uuid = c[\"URL\"][39:]\n",
    "#     if c[\"D1\"] == \"NA\":\n",
    "#         c[\"normed_D1\"] = None\n",
    "#     else:\n",
    "#         c[\"normed_D1\"] = (float(c[\"D1\"]) - 1) / 3\n",
    "\n",
    "#     turn_getter = Conversation(uuid)\n",
    "    \n",
    "#     for this_turn, next_turn in turn_getter.get_pairwise_turns():\n",
    "\n",
    "#         # calculate_score(this_turn, next_turn, stem=True)\n",
    "#         pass\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49825d3d",
   "metadata": {},
   "source": [
    "### Creating a historgram showing values at a glance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87c99dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# if using a Jupyter notebook, includue:\n",
    "%matplotlib inline\n",
    "\n",
    "plt.hist(df[\"z_D1\"], 10,\n",
    "         density=True,\n",
    "         histtype='bar',\n",
    "         facecolor='g',\n",
    "         alpha=0.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c20127",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df[\"normed_D1\"], 10,\n",
    "         density=True,\n",
    "         histtype='bar',\n",
    "         facecolor='g',\n",
    "         alpha=0.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f156efe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.z_D1.mean())\n",
    "print(df.z_D1.median())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8df13a",
   "metadata": {},
   "source": [
    "### Preprocessing function to be shared by models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60c22fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download()\n",
    "\n",
    "import string \n",
    "s =  set(string.punctuation)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "example = \"\"\"This is a sample sentence,\n",
    "                  showing off the stop words filtration.\"\"\"\n",
    "\n",
    "def preprocess_text(sentence, remove_stopwords=False, lemmatize=False):\n",
    "    word_tokens = word_tokenize(sentence)\n",
    " \n",
    "    # lowercase\n",
    "    word_tokens = [w.lower() for w in word_tokens]\n",
    "    \n",
    "    # strip punctuation\n",
    "    word_tokens = [w for w in word_tokens if w not in s]\n",
    "    \n",
    "    if remove_stopwords is True:\n",
    "        word_tokens = [w for w in word_tokens if not w in stop_words]\n",
    "    \n",
    "    if lemmatize is True:\n",
    "        word_tokens = [lemmatizer.lemmatize(w) for w in word_tokens]\n",
    "    \n",
    "    return word_tokens\n",
    "\n",
    "\n",
    "# preprocess_text(example)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041c453f",
   "metadata": {},
   "source": [
    "### Baseline Word-Overlapping Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503a76c3",
   "metadata": {},
   "source": [
    "#### function calculating wordverlap between pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb36e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that calculate word overlapping\n",
    "\n",
    "def calculate_wordoverlap_score(\n",
    "    this_turn, next_turn, remove_stopwords=False, lemmatize=False, min_toks=5):\n",
    "    \n",
    "    this_turn_toks = set(preprocess_text(this_turn, remove_stopwords, lemmatize))\n",
    "    next_turn_toks = set(preprocess_text(next_turn, remove_stopwords, lemmatize))\n",
    "    \n",
    "    overlap = 0\n",
    "    \n",
    "    if len(this_turn_toks) < min_toks:\n",
    "        return None\n",
    "    \n",
    "    for token in this_turn_toks:\n",
    "\n",
    "        if token in next_turn_toks:\n",
    "            overlap +=1\n",
    "    score = overlap / len(this_turn_toks)\n",
    "        \n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7799e09f",
   "metadata": {},
   "source": [
    "#### function getting the baseline prediction (with preprocessing swtich and final calculating switch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417471b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from statistics import mean\n",
    "from conversations import Conversation\n",
    "\n",
    "def get_baseline_prediction(uuid, remove_stopwords=False, lemmatize=False, min_toks=5, aggregate=mean):\n",
    "    turn_getter = Conversation(uuid) # getting conversations from the XML files using Simon's function\n",
    "    \n",
    "    scores = [\n",
    "        calculate_wordoverlap_score(this_turn, next_turn,  remove_stopwords, lemmatize, min_toks)\n",
    "        for this_turn, next_turn in turn_getter.get_pairwise_turns()\n",
    "    ]\n",
    "    \n",
    "    scores = [score for score in scores if score is not None]\n",
    "    \n",
    "    if not len(scores):\n",
    "        return None\n",
    "    \n",
    "    return aggregate(scores)\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835b3403",
   "metadata": {},
   "source": [
    "#### raw text without punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497ad910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating baseline, raw text without punctuation predictions, aggregate by mean\n",
    "# adding baseline_raw prediction to the dataframe\n",
    "\n",
    "df = df.assign(baseline_raw_prediction = df.uuid.apply(get_baseline_prediction))\n",
    "\n",
    "df = df.assign(z_baseline_raw = stats.zscore(df[\"baseline_raw_prediction\"], nan_policy='omit'))\n",
    "\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fc0f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df[\"baseline_raw_prediction\"], 10,\n",
    "         density=True,\n",
    "         histtype='bar',\n",
    "         facecolor='r',\n",
    "         alpha=0.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f194fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.baseline_raw_prediction.mean())\n",
    "print(df.baseline_raw_prediction.median())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b00079",
   "metadata": {},
   "source": [
    "#### replace mean value with max value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c278316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding baseline_raw prediction aggregated by max to the dataframe\n",
    "df = df.assign(baseline_raw_max_prediction = df.uuid.apply(lambda x: get_baseline_prediction(x, aggregate=max)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295db882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(df[\"baseline_raw_max_prediction\"], 10,\n",
    "#          density=True,\n",
    "#          histtype='bar',\n",
    "#          facecolor='r',\n",
    "#          alpha=0.5)\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91db7676",
   "metadata": {},
   "source": [
    "#### replacing mean with mean softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3881f5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "\n",
    "def mean_softmax(vals):\n",
    "    return mean(softmax(vals))\n",
    "\n",
    "# adding baseline_raw prediction to the dataframe\n",
    "df = df.assign(baseline_raw_softmax_mean_prediction = \n",
    "               df.uuid.apply(lambda x: get_baseline_prediction(x, aggregate=mean_softmax)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e9eeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### replacing mean with max softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d16528",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_softmax(vals):\n",
    "    return max(softmax(vals))\n",
    "\n",
    "# adding baseline_raw prediction to the dataframe\n",
    "df = df.assign(baseline_raw_softmax_max_prediction = \n",
    "               df.uuid.apply(lambda x: get_baseline_prediction(x, aggregate=max_softmax)))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f59102a",
   "metadata": {},
   "source": [
    "#### stop words removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5820ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating baseline with stop words removed prediction, aggregated by max\n",
    "# there are issues with empty turns after moving stop words!\n",
    "\n",
    "# adding baseline prediction to the dataframe\n",
    "df = df.assign(baseline_stprm_prediction =\n",
    "               df.uuid.apply(lambda uuid: get_baseline_prediction(uuid, remove_stopwords=True)))\n",
    "\n",
    "# using max instead of mean\n",
    "df = df.assign(baseline_stprm_max_prediction =\n",
    "               df.uuid.apply(lambda uuid: get_baseline_prediction(uuid, remove_stopwords=True, aggregate=max)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035baad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(df[\"baseline_stprm_prediction\"], 10,\n",
    "#          density=True,\n",
    "#          histtype='bar',\n",
    "#          facecolor='y',\n",
    "#          alpha=0.5)\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3535d2d5",
   "metadata": {},
   "source": [
    "#### lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0382ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating baseline using lemmatization predictions\n",
    "\n",
    "# adding baseline prediction to the dataframe\n",
    "df = df.assign(baseline_lemmatized_prediction =\n",
    "               df.uuid.apply(lambda uuid: get_baseline_prediction(uuid, lemmatize=True)))\n",
    "\n",
    "df = df.assign(baseline_lemmatized_max_prediction =\n",
    "               df.uuid.apply(lambda uuid: get_baseline_prediction(uuid, lemmatize=True, aggregate=max)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58549b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df[\"baseline_lemmatized_prediction\"], 10,\n",
    "         density=True,\n",
    "         histtype='bar',\n",
    "         facecolor='c',\n",
    "         alpha=0.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cf864c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing stopwords and lemmatizing\n",
    "df = df.assign(baseline_all_prediction =\n",
    "               df.uuid.apply(lambda uuid: get_baseline_prediction(uuid, lemmatize=True, remove_stopwords=True)))\n",
    "\n",
    "df = df.assign(baseline_all_max_prediction =\n",
    "               df.uuid.apply(lambda uuid: get_baseline_prediction(uuid, lemmatize=True, remove_stopwords=True, aggregate=max)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5bb78c",
   "metadata": {},
   "source": [
    "### cosine similarity between sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0288c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Program to measure the similarity between \n",
    "# two sentences using cosine similarity.\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def calculate_sentence_similarity(\n",
    "    this_turn, next_turn, remove_stopwords=False, lemmatize=False, min_toks=5):\n",
    "    \n",
    "    this_turn_toks = set(preprocess_text(this_turn, remove_stopwords, lemmatize))\n",
    "    next_turn_toks = set(preprocess_text(next_turn, remove_stopwords, lemmatize))\n",
    "        \n",
    "    if len(this_turn_toks) < min_toks or len(next_turn_toks) < min_toks:\n",
    "        return None\n",
    "    \n",
    "    s1 = []\n",
    "    s2 = []\n",
    "    # form a set containing keywords of both strings \n",
    "    rvector = this_turn_toks.union(next_turn_toks) \n",
    "    \n",
    "    for w in rvector:\n",
    "        if w in this_turn_toks:\n",
    "            s1.append(1) # create a vector\n",
    "        else:\n",
    "            s1.append(0)\n",
    "        if w in next_turn_toks:\n",
    "            s2.append(1)\n",
    "        else: \n",
    "            s2.append(0)\n",
    "    c = 0\n",
    "    \n",
    "    # cosine formula \n",
    "    for i in range(len(rvector)):\n",
    "            c+= s1[i]*s2[i]\n",
    "    cosine = c / float((sum(s1)*sum(s2))**0.5)\n",
    "    return cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69cad142",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_similarity_prediction(uuid, remove_stopwords=False, lemmatize=False, min_toks=5, aggregate=mean):\n",
    "    turn_getter = Conversation(uuid) # getting conversations from the XML files using Simon's function\n",
    "    \n",
    "    scores = [\n",
    "        calculate_sentence_similarity(this_turn, next_turn,  remove_stopwords, lemmatize, min_toks)\n",
    "        for this_turn, next_turn in turn_getter.get_pairwise_turns()\n",
    "    ]\n",
    "    \n",
    "    scores = [score for score in scores if score is not None]\n",
    "    \n",
    "    if not len(scores):\n",
    "        return None\n",
    "    \n",
    "    return aggregate(scores)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36802cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.assign(sentence_similarity_prediction =\n",
    "               df.uuid.apply(lambda uuid: get_sentence_similarity_prediction(uuid)))\n",
    "\n",
    "df = df.assign(sentence_similarity_max_prediction =\n",
    "               df.uuid.apply(lambda uuid: get_sentence_similarity_prediction(uuid, aggregate=max)))\n",
    "\n",
    "df = df.assign(sentence_similarity_stprm_prediction =\n",
    "               df.uuid.apply(lambda uuid: get_sentence_similarity_prediction(uuid, remove_stopwords=True)))\n",
    "\n",
    "df = df.assign(sentence_similarity_stprm_max_prediction =\n",
    "               df.uuid.apply(lambda uuid: get_sentence_similarity_prediction(uuid, remove_stopwords=True, aggregate=max)))\n",
    "\n",
    "df = df.assign(sentence_similarity_lemma_prediction =\n",
    "               df.uuid.apply(lambda uuid: get_sentence_similarity_prediction(uuid, lemmatize = True)))\n",
    "\n",
    "df = df.assign(sentence_similarity_lemma_max_prediction =\n",
    "               df.uuid.apply(lambda uuid: get_sentence_similarity_prediction(uuid, lemmatize = True, aggregate=max)))\n",
    "\n",
    "df = df.assign(sentence_similarity_all_prediction =\n",
    "               df.uuid.apply(lambda uuid: get_sentence_similarity_prediction(uuid, remove_stopwords=True, lemmatize=True)))\n",
    "\n",
    "df = df.assign(sentence_similarity_all_max_prediction =\n",
    "               df.uuid.apply(lambda uuid: get_sentence_similarity_prediction(uuid, remove_stopwords=True, lemmatize=True, aggregate=max)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519cbb9b",
   "metadata": {},
   "source": [
    "### checking correlations between human labels and baseline models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc32d3a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(df[\"z_D1\"].corr(df[\"z_baseline_raw\"]))\n",
    "\n",
    "print(\"baseline-raw: \", df[\"normed_D1\"].corr(df[\"baseline_raw_prediction\"]))\n",
    "\n",
    "print(\"baseline-raw-max: \",df[\"normed_D1\"].corr(df[\"baseline_raw_max_prediction\"]))\n",
    "\n",
    "print(\"baseline-stprm: \",df[\"normed_D1\"].corr(df[\"baseline_stprm_prediction\"]))\n",
    "\n",
    "print(\"baseline-stprm_max: \",df[\"normed_D1\"].corr(df[\"baseline_stprm_max_prediction\"]))\n",
    "\n",
    "print(\"baseline-lemma: \",df[\"normed_D1\"].corr(df[\"baseline_lemmatized_prediction\"]))\n",
    "\n",
    "print(\"baseline-lemma_max: \",df[\"normed_D1\"].corr(df[\"baseline_lemmatized_max_prediction\"]))\n",
    "\n",
    "print(\"baseline-all: \", df[\"normed_D1\"].corr(df[\"baseline_all_prediction\"]))\n",
    "\n",
    "print(\"baseline-max: \",df[\"normed_D1\"].corr(df[\"baseline_all_max_prediction\"]))\n",
    "\n",
    "print(\"ss: \",df[\"normed_D1\"].corr(df[\"sentence_similarity_prediction\"]))\n",
    "\n",
    "print(\"ss-max: \",df[\"normed_D1\"].corr(df[\"sentence_similarity_max_prediction\"]))\n",
    "\n",
    "print(\"ss-stprm: \",df[\"normed_D1\"].corr(df[\"sentence_similarity_stprm_prediction\"]))\n",
    "\n",
    "print(\"ss-stprm-max: \",df[\"normed_D1\"].corr(df[\"sentence_similarity_stprm_max_prediction\"]))\n",
    "\n",
    "print(\"ss-lemma: \",df[\"normed_D1\"].corr(df[\"sentence_similarity_lemma_prediction\"]))\n",
    "\n",
    "print(\"ss-lemma-max: \",df[\"normed_D1\"].corr(df[\"sentence_similarity_lemma_max_prediction\"]))\n",
    "\n",
    "print(\"ss-all: \",df[\"normed_D1\"].corr(df[\"sentence_similarity_all_prediction\"]))\n",
    "\n",
    "print(\"ss-all-max: \",df[\"normed_D1\"].corr(df[\"sentence_similarity_all_max_prediction\"]))\n",
    "\n",
    "# print(df[\"normed_D1\"].corr(df[\"baseline_raw_softmax_mean_prediction\"]))\n",
    "\n",
    "# print(df[\"normed_D1\"].corr(df[\"baseline_raw_softmax_max_prediction\"]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3f85cf",
   "metadata": {},
   "source": [
    "### creating confusion matrix and evaluation (F-score, macro F-score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9d822f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "from nltk.metrics.distance import edit_distance\n",
    "from nltk.translate import bleu_score\n",
    "import numpy as np\n",
    "# import pandas as pd\n",
    "import scipy.stats\n",
    "from sklearn import metrics\n",
    "# import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b4bf8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign label\n",
    "\n",
    "def assign_label(value):\n",
    "    if value > 0.5:\n",
    "        return \"High\"\n",
    "    return \"Low\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2b54a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### assigning labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f882b5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating labels\n",
    "\n",
    "df = df.assign(normed_D1_label =\n",
    "               df.normed_D1.apply(assign_label))\n",
    "\n",
    "df = df.assign(baseline_raw_prediction_label =\n",
    "               df.baseline_raw_prediction.apply(assign_label))\n",
    "\n",
    "df = df.assign(baseline_raw_max_prediction_label =\n",
    "               df.baseline_raw_max_prediction.apply(assign_label))\n",
    "\n",
    "df = df.assign(baseline_lemmatized_prediction_label =\n",
    "               df.baseline_lemmatized_prediction.apply(assign_label))\n",
    "\n",
    "df = df.assign(baseline_lemmatized_max_prediction_label =\n",
    "               df.baseline_lemmatized_max_prediction.apply(assign_label))\n",
    "\n",
    "df = df.assign(baseline_stprm_prediction_label =\n",
    "               df.baseline_stprm_prediction.apply(assign_label))\n",
    "\n",
    "df = df.assign(baseline_stprm_max_prediction_label =\n",
    "               df.baseline_stprm_max_prediction.apply(assign_label))\n",
    "\n",
    "df = df.assign(baseline_all_prediction_label =\n",
    "               df.baseline_all_prediction.apply(assign_label))\n",
    "\n",
    "df = df.assign(baseline_all_max_prediction_label =\n",
    "               df.baseline_all_max_prediction.apply(assign_label))\n",
    "\n",
    "df = df.assign(sentence_similarity_prediction_label =\n",
    "               df.sentence_similarity_prediction.apply(assign_label))\n",
    "\n",
    "df = df.assign(sentence_similarity_max_prediction_label =\n",
    "               df.sentence_similarity_max_prediction.apply(assign_label))\n",
    "\n",
    "df = df.assign(sentence_similarity_stprm_prediction_label =\n",
    "               df.sentence_similarity_stprm_prediction.apply(assign_label))\n",
    "\n",
    "df = df.assign(sentence_similarity_stprm_max_prediction_label =\n",
    "               df.sentence_similarity_stprm_max_prediction.apply(assign_label))\n",
    "\n",
    "df = df.assign(sentence_similarity_lemma_prediction_label =\n",
    "               df.sentence_similarity_lemma_prediction.apply(assign_label))\n",
    "\n",
    "df = df.assign(sentence_similarity_lemma_max_prediction_label =\n",
    "               df.sentence_similarity_lemma_max_prediction.apply(assign_label))\n",
    "\n",
    "df = df.assign(sentence_similarity_all_prediction_label =\n",
    "               df.sentence_similarity_all_prediction.apply(assign_label))\n",
    "\n",
    "df = df.assign(sentence_similarity_all_max_prediction_label =\n",
    "               df.sentence_similarity_all_max_prediction.apply(assign_label))\n",
    "\n",
    "# df = df.assign(baseline_raw_softmax_mean_prediction_label =\n",
    "#                df.baseline_raw_softmax_mean_prediction.apply(assign_label))\n",
    "\n",
    "\n",
    "# df = df.assign(baseline_raw_softmax_max_prediction_label =\n",
    "#                df.baseline_raw_softmax_max_prediction.apply(assign_label))\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b760f860",
   "metadata": {},
   "source": [
    "### generating confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41881a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# raw\n",
    "X = df.normed_D1_label\n",
    "Y_raw = df.baseline_raw_prediction_label\n",
    "Y_raw_max = df.baseline_raw_max_prediction_label\n",
    "Y_lemma = df.baseline_lemmatized_prediction_label\n",
    "Y_lemma_max = df.baseline_lemmatized_max_prediction_label\n",
    "Y_stprm = df.baseline_stprm_prediction_label\n",
    "Y_stprm_max = df.baseline_stprm_max_prediction_label\n",
    "Y_all = df.baseline_all_prediction_label\n",
    "Y_all_max = df.baseline_all_max_prediction_label\n",
    "Y_ss = df.sentence_similarity_prediction_label\n",
    "Y_ss_max = df.sentence_similarity_max_prediction_label\n",
    "Y_ss_stprm = df.sentence_similarity_stprm_prediction_label\n",
    "Y_ss_stprm_max = df.sentence_similarity_stprm_max_prediction_label\n",
    "Y_ss_lemma = df.sentence_similarity_lemma_prediction_label\n",
    "Y_ss_lemma_max = df.sentence_similarity_stprm_max_prediction_label\n",
    "Y_ss_all = df.sentence_similarity_all_prediction_label\n",
    "Y_ss_all_max = df.sentence_similarity_all_max_prediction_label\n",
    "\n",
    "# Y_raw_softmax_mean = df.baseline_raw_softmax_mean_prediction_label\n",
    "# Y_raw_softmax_max = df.baseline_raw_softmax_max_prediction_label\n",
    "\n",
    "\n",
    "#Generate the confusion matrix\n",
    "# cm_raw = confusion_matrix(X, Y_raw)\n",
    "# cm_raw_max = confusion_matrix(X, Y_raw_max)\n",
    "# cm_raw_softmax_mean = confusion_matrix(X, Y_raw_softmax_mean)\n",
    "# cm_raw_softmax_max = confusion_matrix(X, Y_raw_softmax_max)\n",
    "# cm_stprm = confusion_matrix(X, Y_stprm)\n",
    "# cm_lemmatized = confusion_matrix(X, Y_lemma)\n",
    "\n",
    "# print(cm_raw)\n",
    "# print(cm_raw_max)\n",
    "# print(cm_raw_softmax_mean)\n",
    "# print(cm_raw_softmax_max)\n",
    "# print(cm_lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2d9bf0",
   "metadata": {},
   "source": [
    "### calculating F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c69a253",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# precision = precision_score(Y_raw, X, average='binary', pos_label=\"High\")\n",
    "# recall = recall_score(Y_raw, X, average='binary', pos_label=\"High\")\n",
    "\n",
    "f1_raw = f1_score(Y_raw, X, average='binary', pos_label=\"High\")\n",
    "f1_raw_max = f1_score(Y_raw_max, X, average='binary', pos_label=\"High\")\n",
    "# f1_raw_softmax_mean = f1_score(Y_raw_softmax_mean, X, average='binary', pos_label=\"High\")\n",
    "# f1_raw_softmax_max = f1_score(Y_raw_softmax_max, X, average='binary', pos_label=\"High\")\n",
    "f1_lemma = f1_score(Y_lemma, X, average='binary', pos_label=\"High\")\n",
    "f1_lemma_max = f1_score(Y_lemma_max, X, average='binary', pos_label=\"High\")\n",
    "f1_stprm = f1_score(Y_stprm, X, average='binary', pos_label=\"High\")\n",
    "f1_stprm_max = f1_score(Y_stprm_max, X, average='binary', pos_label=\"High\")\n",
    "f1_all = f1_score(Y_all, X, average='binary', pos_label=\"High\")\n",
    "f1_all_max = f1_score(Y_all_max, X, average='binary', pos_label=\"High\")\n",
    "f1_ss = f1_score(Y_ss, X, average='binary', pos_label=\"High\")\n",
    "f1_ss_max = f1_score(Y_ss_max, X, average='binary', pos_label=\"High\")\n",
    "f1_ss_stprm = f1_score(Y_ss_stprm, X, average='binary', pos_label=\"High\")\n",
    "f1_ss_stprm_max = f1_score(Y_ss_stprm_max, X, average='binary', pos_label=\"High\")\n",
    "f1_ss_lemma = f1_score(Y_ss_lemma, X, average='binary', pos_label=\"High\")\n",
    "f1_ss_lemma_max = f1_score(Y_ss_lemma_max, X, average='binary', pos_label=\"High\")\n",
    "f1_ss_all = f1_score(Y_ss_all, X, average='binary', pos_label=\"High\")\n",
    "f1_ss_all_max = f1_score(Y_ss_all_max, X, average='binary', pos_label=\"High\")\n",
    "\n",
    "\n",
    "print(\"raw: \", f1_raw)\n",
    "print(\"raw-max: \", f1_raw_max)\n",
    "# f1_raw_softmax_mean\n",
    "# f1_raw_softmax_max\n",
    "print(\"raw-lemma: \",f1_lemma)\n",
    "print(\"raw-lemma-max: \",f1_lemma_max)\n",
    "print(\"raw-stprm: \",f1_stprm)\n",
    "print(\"raw-stprm-max: \",f1_stprm_max)\n",
    "print(\"raw-all: \",f1_all)\n",
    "print(\"raw-all-max: \",f1_all_max)\n",
    "print(\"ss: \",f1_ss)\n",
    "print(\"ss-max: \",f1_ss_max)\n",
    "print(\"ss-stprm: \",f1_ss_stprm)\n",
    "print(\"ss-stprm-max: \",f1_ss_stprm_max)\n",
    "print(\"ss-lemma: \",f1_ss_lemma)\n",
    "print(\"ss-lemma-max: \",f1_ss_lemma_max)\n",
    "print(\"ss-all: \",f1_ss_all)\n",
    "print(\"ss-all-max: \",f1_ss_all_max)\n",
    "\n",
    "'''interesting insight, removing stopwords does not help with either word-overlap or sentence similarity;\n",
    "lemmatization does not help with word overlap, but has some benefits in sentence similarity.\n",
    "Also, raw text word-overlap model still presens'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54adaad9",
   "metadata": {},
   "source": [
    "### mean static representaions from Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbee5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions from the cs224u VSM module\n",
    "\n",
    "def hf_encode(text, tokenizer, add_special_tokens=False):\n",
    "    \"\"\"\n",
    "    Get the indices for the tokens in `text` according to `tokenizer`.\n",
    "    If no tokens can be obtained from `text`, then the tokenizer.unk_token`\n",
    "    is used as the only token.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text: str\n",
    "\n",
    "    tokenizer: Hugging Face tokenizer\n",
    "\n",
    "    add_special_tokens : bool\n",
    "        A Hugging Face parameter to the tokenizer.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    torch.Tensor of shape `(1, m)`\n",
    "        A batch of 1 example of `m` tokens`, where `m` is determined\n",
    "        by `text` and the nature of `tokenizer`.\n",
    "\n",
    "    \"\"\"\n",
    "    encoding = tokenizer.encode(\n",
    "        text,\n",
    "        add_special_tokens=add_special_tokens,\n",
    "        return_tensors='pt')\n",
    "    if encoding.shape[1] == 0:\n",
    "        text = tokenizer.unk_token\n",
    "        encoding = torch.tensor([[tokenizer.vocab[text]]])\n",
    "    return encoding\n",
    "\n",
    "\n",
    "def hf_represent(batch_ids, model, layer=-1):\n",
    "    \"\"\"\n",
    "    Encode a batch of sequences of ids using a Hugging Face\n",
    "    Transformer-based model `model`. The model's `forward` method is\n",
    "    `output_hidden_states=True`, and we get the hidden states from\n",
    "    `layer`.\n",
    "\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    batch_ids : iterable, shape (n_examples, n_tokens)\n",
    "        Sequences of indices into the model vocabulary.\n",
    "\n",
    "    model : Hugging Face transformer model\n",
    "\n",
    "    layer : int\n",
    "        The layer to return. This will get all the hidden states at\n",
    "        this layer. `layer=0` gives the embedding, and `layer=-1`\n",
    "        gives the final output states.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tensor of shape `(n_examples, n_tokens, n_dimensions)`\n",
    "       where `n_dimensions` is the dimensionality of the\n",
    "       Transformer model\n",
    "\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        reps = model(batch_ids, output_hidden_states=True)\n",
    "        return reps.hidden_states[layer]\n",
    "    \n",
    "    \n",
    "def mean_pooling(hidden_states):\n",
    "    \"\"\"\n",
    "    Get the mean along `axis=1` of a Tensor.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    hidden_states : torch.Tensor, shape `(k, m, n)`\n",
    "        Where `k` is the number of examples, `m` is the number of vectors\n",
    "        for each example, and `n` is dimensionality of each vector.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    torch.Tensor of dimension `(k, n)`.\n",
    "\n",
    "    \"\"\"\n",
    "    _check_pooling_dimensionality(hidden_states)\n",
    "    return torch.mean(hidden_states, axis=1)\n",
    "\n",
    "\n",
    "def _check_pooling_dimensionality(hidden_states):\n",
    "     if not len(hidden_states.shape) == 3:\n",
    "        raise ValueError(\n",
    "            \"The input to the pooling function should have 3 dimensions: \"\n",
    "            \"it's a batch of k examples, where each example has m vectors, \"\n",
    "            \"each of dimensionality n. The function will pool the vectors \"\n",
    "            \"for each example, returning a Tensor of shape (k, n).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901c9ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "bert_weights_name = 'bert-base-uncased'\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(bert_weights_name)\n",
    "bert_model = BertModel.from_pretrained(bert_weights_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398dc000",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def get_static_rep_from_bert(word):\n",
    "    # derived from vsm_04_contextualreps.ipynb\n",
    "    subtok_ids = hf_encode(word, bert_tokenizer)\n",
    "    subtok_reps = hf_represent(subtok_ids, bert_model, layer=-1)\n",
    "    subtok_pooled = mean_pooling(subtok_reps)\n",
    "    return subtok_pooled\n",
    "\n",
    "\n",
    "def get_score_for_pair_with_similarity_threshold(this_turn, next_turn, threshold=0.1, min_tokens=5):\n",
    "\n",
    "    overlap = 0\n",
    "    cosine_similarity = torch.nn.CosineSimilarity(dim=0)\n",
    "    \n",
    "    # turn each utterance into a list of tensor representations\n",
    "    this_turn_tokens = [reps_lookup[t] for t in set(preprocess_text(this_turn))]\n",
    "    next_turn_tokens = [reps_lookup[t] for t in set(preprocess_text(next_turn))]\n",
    "\n",
    "    if len(this_turn_tokens) < min_tokens:\n",
    "        return None\n",
    "    \n",
    "    # calculate cosine similarity measures for each pair of token representations\n",
    "    for token1 in this_turn_tokens:\n",
    "        for token2 in next_turn_tokens:\n",
    "            cos =  torch.mean(cosine_similarity(token1, token2), axis=0)\n",
    "            # if the cosine similarity exceeds the (arbitary) threshold, increment the count\n",
    "            if cos > threshold:\n",
    "                overlap +=1\n",
    "\n",
    "    # divide the count by the number of tokens in the first turn, for some degree of scaling\n",
    "    score = overlap / len(set(this_turn.split()))\n",
    "        \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476ea6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imporitng source data with samples in ela and science, between 2 students only\n",
    "    \n",
    "import csv\n",
    "conversations = []\n",
    "\n",
    "with open(\"focal_ela_science.csv\", \"r\", encoding = \"UTF-8\") as d:\n",
    "    reader = csv.DictReader(d)\n",
    "    for row in reader:\n",
    "        conversations.append(row)\n",
    "\n",
    "# conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccea3415",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "def get_all_tokens():\n",
    "    toks = set()\n",
    "    for c in conversations:\n",
    "        uuid = c[\"URL\"][39:]\n",
    "        turn_getter = Conversation(uuid)\n",
    "        toks.update(list(itertools.chain(\n",
    "            *[preprocess_text(turn) for spkr, turn in turn_getter.get_turns() if turn is not None]\n",
    "        )))\n",
    "    return toks\n",
    "\n",
    "all_toks = get_all_tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277c960a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time reps_lookup = {tok: get_static_rep_from_bert(tok) for tok in all_toks}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b073ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import logging\n",
    "# from importlib import reload\n",
    "# from statistics import mean\n",
    "\n",
    "# reload(logging)\n",
    "# logging.basicConfig(\n",
    "#     level=logging.INFO,\n",
    "#     format=\"%(asctime)s: %(message)s\",\n",
    "#     datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "# )\n",
    "\n",
    "# for i, c in enumerate(conversations):\n",
    "#     if c[\"D1\"] == \"NA\":\n",
    "#         c[\"normed_D1\"] = None\n",
    "#     else:\n",
    "#         c[\"normed_D1\"] = (float(c[\"D1\"]) - 1) / 3    \n",
    "       \n",
    "#     uuid = c[\"URL\"][39:]\n",
    "#     conv = Conversation(uuid)\n",
    "\n",
    "#     scores = []\n",
    "#     for this_turn, next_turn in conv.get_pairwise_turns():\n",
    "#         score = get_score_for_pair_with_similarity_threshold(this_turn, next_turn)\n",
    "#         scores.append(score)\n",
    "\n",
    "#     score = mean([score for score in scores if score is not None])\n",
    "    \n",
    "#     label = (\"low\", \"high\")[c[\"normed_D1\"] > 0.5 if c[\"normed_D1\"] is not None else 0]\n",
    "\n",
    "#     logging.info(\"%2d: %s %f\", i, label, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9f52fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_prediction(uuid, min_toks=5, aggregate=mean):\n",
    "    turn_getter = Conversation(uuid) # getting conversations from the XML files using Simon's function\n",
    "    \n",
    "    scores = [\n",
    "        get_score_for_pair_with_similarity_threshold(this_turn, next_turn)\n",
    "        for this_turn, next_turn in turn_getter.get_pairwise_turns()\n",
    "    ]\n",
    "    \n",
    "    scores = [score for score in scores if score is not None]\n",
    "    \n",
    "    if not len(scores):\n",
    "        return None\n",
    "    \n",
    "    return aggregate(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0da8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.assign(bert_prediction =\n",
    "               df.uuid.apply(lambda uuid: get_bert_prediction(uuid)))\n",
    "\n",
    "df = df.assign(bert_max_prediction =\n",
    "               df.uuid.apply(lambda uuid: get_bert_prediction(uuid, aggregate=max)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e223a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation\n",
    "\n",
    "print(df[\"normed_D1\"].corr(df[\"bert_prediction\"]))\n",
    "print(df[\"normed_D1\"].corr(df[\"bert_max_prediction\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c68cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.bert_prediction.max())\n",
    "print(df.bert_prediction.min())\n",
    "print(df.bert_prediction.median())\n",
    "print(df.bert_max_prediction.max())\n",
    "print(df.bert_max_prediction.min())\n",
    "print(df.bert_max_prediction.median())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950b7776",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df[\"bert_prediction\"], 10,\n",
    "         density=True,\n",
    "         histtype='bar',\n",
    "         facecolor='g',\n",
    "         alpha=0.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f314379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign label for BERT\n",
    "\n",
    "def assign_bert_label(value, threshold):\n",
    "    if value > threshold: # using the median number for the dataset\n",
    "        return \"High\"\n",
    "    return \"Low\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11622db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assing label\n",
    "df = df.assign(bert_prediction_label =\n",
    "               df.bert_prediction.apply(lambda x: assign_bert_label(x, threshold=11)))\n",
    "\n",
    "df = df.assign(bert_max_prediction_label =\n",
    "               df.bert_prediction.apply(lambda x: assign_bert_label(x, threshold=21)))\n",
    "\n",
    "#Generate the confusion matrix\n",
    "Y_bert = df.bert_prediction_label\n",
    "Y_bert_max = df.bert_max_prediction_label\n",
    "# cm_bert = confusion_matrix(X, Y_bert)\n",
    "\n",
    "# calculate f1 score\n",
    "f1_bert = f1_score(Y_bert, X, average='binary', pos_label=\"High\")\n",
    "f1_bert_max = f1_score(Y_bert_max, X, average='binary', pos_label=\"High\")\n",
    "\n",
    "print(\"bert: \", f1_bert)\n",
    "print(\"bert: \", f1_bert_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2592ec0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189f5e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.assign(z_baseline_raw_max = stats.zscore(df[\"baseline_raw_max_prediction\"], nan_policy='omit'))\n",
    "df = df.assign(z_baseline_lemma_max = stats.zscore(df[\"baseline_lemmatized_max_prediction\"], nan_policy='omit'))\n",
    "df = df.assign(z_sentence_similarity_raw_max = stats.zscore(df[\"sentence_similarity_max_prediction\"], nan_policy='omit'))\n",
    "df = df.assign(z_bert_prediction = stats.zscore(df[\"bert_prediction\"], nan_policy='omit'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487278b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"focal_ela_science_NLP.csv\", encoding='utf-8', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43d237f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
